{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjqxiR_SYG7M",
        "outputId": "1d5716f3-365f-48a7-b6bd-95b63ee3e866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06aItOs6YJDn",
        "outputId": "42f08711-4d19-4046-dc57-c2f56b61897e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.68)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.4)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.9)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLc1jnU9Y4W8",
        "outputId": "70ac4496-8cd9-45bf-8648-3a0de0b53a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwWqCBEVX12z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fitz\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('o_key')\n",
        "\n",
        "qa_chain = None"
      ],
      "metadata": {
        "id": "H77wZ_Ogg6I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading our PDF\n",
        "def load_pdf_text(pdf_path):\n",
        "  doc = fitz.open(pdf_path)\n",
        "  full_text = \"\"\n",
        "  for page in doc:\n",
        "    text = page.get_text()\n",
        "    full_text += text\n",
        "  return full_text"
      ],
      "metadata": {
        "id": "1nh2BdXEYCCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split_text\n",
        "def split_text(text):\n",
        "  splitter = RecursiveCharacterTextSplitter(chunk_size=300 , chunk_overlap=100)\n",
        "  docs = splitter.create_documents([text])\n",
        "  return docs"
      ],
      "metadata": {
        "id": "0Pet4n_cYSWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create vector store\n",
        "#each chunk of text converted into vector using OpenAI embeddings\n",
        "#FAISS stores those vectors and allows fast similarity search\n",
        "def create_vector_store(docs):\n",
        "  embeddings = OpenAIEmbeddings()\n",
        "  vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "  return vectorstore"
      ],
      "metadata": {
        "id": "doj5WHO7YU1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup RAG pipeline\n",
        "#retriever - pulls top 3 most similar chunks to question\n",
        "#llm - GPT model that will generate an answer using those chunks\n",
        "#RetrievalQA - combines them into a Question answering system\n",
        "\n",
        "def setup_rag_qa(vectorstore):\n",
        "  retriever = vectorstore.as_retriever(search_type=\"similarity\" , search_kwargs={\"k\":3})\n",
        "  llm = ChatOpenAI(temperature=0.3, model=\"gpt-4.1-nano\")\n",
        "  rag_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                          retriever=retriever)\n",
        "  return rag_chain"
      ],
      "metadata": {
        "id": "Fat63L1ZYYAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle PDF upload and process\n",
        "\n",
        "def upload_pdf(file):\n",
        "  global qa_chain\n",
        "  if file is None:\n",
        "    return \"No file uploaded\"\n",
        "\n",
        "  try:\n",
        "    text = load_pdf_text(file.name)\n",
        "    docs = split_text(text)\n",
        "    vectorstore = create_vector_store(docs)\n",
        "    qa_chain = setup_rag_qa(vectorstore)\n",
        "    return \"PDF uploaded and processed successfully\"\n",
        "  except Exception as e:\n",
        "    return f\"Error : {str(e)}\""
      ],
      "metadata": {
        "id": "a_KWP3ZMYf2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Handle question input and answers\n",
        "def ask_question(query):\n",
        "  if qa_chain is None:\n",
        "    return \"Please upload the PDF first\"\n",
        "  try:\n",
        "    result = qa_chain.run(query)\n",
        "    return result\n",
        "  except Exception as e:\n",
        "    return f\"Error : {str(e)}\""
      ],
      "metadata": {
        "id": "Q6soLoAoaNqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradio UI\n",
        "with gr.Blocks() as ui_demo:\n",
        "  gr.Markdown(\"# RAG assistant with GPT\")\n",
        "  gr.Markdown(\"Upload a PDF, then ask questions from its content. We are using GPT + FAISS + langchain\")\n",
        "\n",
        "  with gr.Row():\n",
        "    pdf_input = gr.File(label = \"Upload PDF\")\n",
        "    upload_status = gr.Textbox(label = \"Upload Status\" , interactive=False)\n",
        "\n",
        "  pdf_input.change(fn=upload_pdf , inputs=pdf_input , outputs=upload_status)\n",
        "\n",
        "  with gr.Row():\n",
        "    question_input = gr.Textbox(label=\"Ask few Questions\")\n",
        "    answer_output = gr.Textbox(label=\"Answer\")\n",
        "\n",
        "  question_input.submit(fn=ask_question , inputs=question_input , outputs=answer_output)\n",
        "\n",
        "\n",
        "ui_demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "pb922POAauvr",
        "outputId": "d10a4bef-8695-4da2-f971-d617411c5784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://204d0d6f5f70b8e0a9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://204d0d6f5f70b8e0a9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "import openai\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "import tempfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import PyPDF2\n",
        "import docx\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self):\n",
        "        self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.documents = []\n",
        "        self.document_embeddings = None\n",
        "        self.index = None\n",
        "        self.openai_client = None\n",
        "\n",
        "    def setup_openai(self, api_key: str):\n",
        "        \"\"\"Setup OpenAI client with API key\"\"\"\n",
        "        if api_key:\n",
        "            self.openai_client = openai.OpenAI(api_key=api_key)\n",
        "            return \"‚úÖ OpenAI API key configured successfully!\"\n",
        "        return \"‚ùå Please provide a valid API key\"\n",
        "\n",
        "    def extract_text_from_file(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text from various file formats\"\"\"\n",
        "        text = \"\"\n",
        "        file_extension = Path(file_path).suffix.lower()\n",
        "\n",
        "        try:\n",
        "            if file_extension == '.pdf':\n",
        "                with open(file_path, 'rb') as file:\n",
        "                    pdf_reader = PyPDF2.PdfReader(file)\n",
        "                    for page in pdf_reader.pages:\n",
        "                        text += page.extract_text()\n",
        "\n",
        "            elif file_extension == '.docx':\n",
        "                doc = docx.Document(file_path)\n",
        "                for paragraph in doc.paragraphs:\n",
        "                    text += paragraph.text + \"\\n\"\n",
        "\n",
        "            elif file_extension == '.txt':\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()\n",
        "\n",
        "            else:\n",
        "                return f\"Unsupported file format: {file_extension}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error reading file: {str(e)}\"\n",
        "\n",
        "        return text\n",
        "\n",
        "    def process_documents(self, files: List[Any]) -> str:\n",
        "        \"\"\"Process uploaded documents and create embeddings\"\"\"\n",
        "        if not files:\n",
        "            return \"No files uploaded\"\n",
        "\n",
        "        self.documents = []\n",
        "        processed_files = []\n",
        "\n",
        "        for file in files:\n",
        "            if hasattr(file, 'name'):\n",
        "                file_path = file.name\n",
        "            else:\n",
        "                file_path = str(file)\n",
        "\n",
        "            text = self.extract_text_from_file(file_path)\n",
        "\n",
        "            if text and not text.startswith(\"Error\") and not text.startswith(\"Unsupported\"):\n",
        "                # Split text into chunks\n",
        "                chunks = self.split_text(text, chunk_size=1000, overlap=200)\n",
        "                for i, chunk in enumerate(chunks):\n",
        "                    self.documents.append({\n",
        "                        'text': chunk,\n",
        "                        'source': Path(file_path).name,\n",
        "                        'chunk_id': i\n",
        "                    })\n",
        "                processed_files.append(Path(file_path).name)\n",
        "            else:\n",
        "                processed_files.append(f\"‚ùå {Path(file_path).name}: {text}\")\n",
        "\n",
        "        if self.documents:\n",
        "            # Create embeddings\n",
        "            texts = [doc['text'] for doc in self.documents]\n",
        "            embeddings = self.embeddings_model.encode(texts)\n",
        "\n",
        "            # Create FAISS index\n",
        "            self.document_embeddings = embeddings\n",
        "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "            self.index.add(embeddings.astype('float32'))\n",
        "\n",
        "            return f\"‚úÖ Successfully processed {len(processed_files)} files:\\n\" + \"\\n\".join(processed_files)\n",
        "\n",
        "        return \"‚ùå No documents could be processed\"\n",
        "\n",
        "    def split_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks\"\"\"\n",
        "        chunks = []\n",
        "        start = 0\n",
        "\n",
        "        while start < len(text):\n",
        "            end = start + chunk_size\n",
        "            chunk = text[start:end]\n",
        "            chunks.append(chunk)\n",
        "            start = end - overlap\n",
        "\n",
        "            if start >= len(text):\n",
        "                break\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def search_documents(self, query: str, top_k: int = 3) -> List[Dict]:\n",
        "        \"\"\"Search for relevant documents using semantic similarity\"\"\"\n",
        "        if not self.index or not self.documents:\n",
        "            return []\n",
        "\n",
        "        query_embedding = self.embeddings_model.encode([query])\n",
        "        distances, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
        "\n",
        "        results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx < len(self.documents):\n",
        "                results.append({\n",
        "                    'text': self.documents[idx]['text'],\n",
        "                    'source': self.documents[idx]['source'],\n",
        "                    'score': float(distances[0][i])\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_response(self, query: str, chat_history: List[List[str]]) -> str:\n",
        "        \"\"\"Generate response using RAG with GPT\"\"\"\n",
        "        if not self.openai_client:\n",
        "            return \"‚ùå Please configure your OpenAI API key first\"\n",
        "\n",
        "        if not self.documents:\n",
        "            return \"‚ùå Please upload and process documents first\"\n",
        "\n",
        "        # Search for relevant documents\n",
        "        relevant_docs = self.search_documents(query, top_k=3)\n",
        "\n",
        "        if not relevant_docs:\n",
        "            return \"‚ùå No relevant documents found\"\n",
        "\n",
        "        # Create context from relevant documents\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"Source: {doc['source']}\\nContent: {doc['text']}\"\n",
        "            for doc in relevant_docs\n",
        "        ])\n",
        "\n",
        "        # Create messages for GPT\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
        "                Use the context below to answer the user's question. If the answer cannot be found in the context,\n",
        "                say so clearly. Always cite the source documents when possible.\n",
        "\n",
        "                Context:\n",
        "                {context}\"\"\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Add chat history\n",
        "        for user_msg, assistant_msg in chat_history[-5:]:  # Last 5 exchanges\n",
        "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "            if assistant_msg:\n",
        "                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "\n",
        "        # Add current query\n",
        "        messages.append({\"role\": \"user\", \"content\": query})\n",
        "\n",
        "        try:\n",
        "            response = self.openai_client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=messages,\n",
        "                max_tokens=1000,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            answer = response.choices[0].message.content\n",
        "\n",
        "            # Add source information\n",
        "            sources = list(set([doc['source'] for doc in relevant_docs]))\n",
        "            answer += f\"\\n\\nüìö Sources: {', '.join(sources)}\"\n",
        "\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error generating response: {str(e)}\"\n",
        "\n",
        "# Initialize RAG system\n",
        "rag_system = RAGSystem()\n",
        "\n",
        "# Gradio interface functions\n",
        "def setup_api_key(api_key):\n",
        "    return rag_system.setup_openai(api_key)\n",
        "\n",
        "def process_files(files):\n",
        "    return rag_system.process_documents(files)\n",
        "\n",
        "def chat_function(message, history):\n",
        "    if not message.strip():\n",
        "        return history, \"\"\n",
        "\n",
        "    response = rag_system.generate_response(message, history)\n",
        "    history.append([message, response])\n",
        "    return history, \"\"\n",
        "\n",
        "def clear_chat():\n",
        "    return [], \"\"\n",
        "\n",
        "def clear_documents():\n",
        "    rag_system.documents = []\n",
        "    rag_system.document_embeddings = None\n",
        "    rag_system.index = None\n",
        "    return \"‚úÖ Documents cleared successfully\"\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"RAG-based Q&A System\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ü§ñ RAG-based Q&A System with GPT\n",
        "\n",
        "    Upload multiple documents and ask questions about their content. The system uses:\n",
        "    - **RAG (Retrieval-Augmented Generation)** for finding relevant information\n",
        "    - **GPT** for generating natural language responses\n",
        "    - **Semantic Search** for accurate document retrieval\n",
        "\n",
        "    Supported formats: PDF, DOCX, TXT\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### üîë Setup\")\n",
        "            api_key_input = gr.Textbox(\n",
        "                label=\"OpenAI API Key\",\n",
        "                type=\"password\",\n",
        "                placeholder=\"Enter your OpenAI API key...\"\n",
        "            )\n",
        "            api_key_btn = gr.Button(\"Configure API Key\", variant=\"primary\")\n",
        "            api_key_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "            gr.Markdown(\"### üìÅ Document Upload\")\n",
        "            file_upload = gr.Files(\n",
        "                label=\"Upload Documents\",\n",
        "                file_count=\"multiple\",\n",
        "                file_types=[\".pdf\", \".docx\", \".txt\"]\n",
        "            )\n",
        "            process_btn = gr.Button(\"Process Documents\", variant=\"primary\")\n",
        "            process_status = gr.Textbox(label=\"Processing Status\", interactive=False)\n",
        "\n",
        "            clear_docs_btn = gr.Button(\"Clear Documents\", variant=\"secondary\")\n",
        "            clear_docs_status = gr.Textbox(label=\"Clear Status\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üí¨ Chat Interface\")\n",
        "\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"Q&A Chat\",\n",
        "                height=400,\n",
        "                show_label=True,\n",
        "                container=True\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                msg_input = gr.Textbox(\n",
        "                    label=\"Your Question\",\n",
        "                    placeholder=\"Ask a question about your documents...\",\n",
        "                    scale=4\n",
        "                )\n",
        "                submit_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "            with gr.Row():\n",
        "                clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ### üìñ How to Use:\n",
        "    1. **Configure API Key**: Enter your OpenAI API key\n",
        "    2. **Upload Documents**: Select multiple PDF, DOCX, or TXT files\n",
        "    3. **Process Documents**: Click to extract text and create embeddings\n",
        "    4. **Ask Questions**: Type your questions and get AI-powered responses\n",
        "\n",
        "    ### üîç Features:\n",
        "    - **Multi-file Support**: Upload and process multiple documents simultaneously\n",
        "    - **Semantic Search**: Find relevant information using AI embeddings\n",
        "    - **Context-aware Responses**: GPT generates answers based on document content\n",
        "    - **Source Attribution**: Responses include source document references\n",
        "    - **Interactive Chat**: Maintain conversation context\n",
        "    \"\"\")\n",
        "\n",
        "    # Event handlers\n",
        "    api_key_btn.click(setup_api_key, inputs=[api_key_input], outputs=[api_key_status])\n",
        "    process_btn.click(process_files, inputs=[file_upload], outputs=[process_status])\n",
        "    clear_docs_btn.click(clear_documents, outputs=[clear_docs_status])\n",
        "\n",
        "    msg_input.submit(chat_function, inputs=[msg_input, chatbot], outputs=[chatbot, msg_input])\n",
        "    submit_btn.click(chat_function, inputs=[msg_input, chatbot], outputs=[chatbot, msg_input])\n",
        "    clear_btn.click(clear_chat, outputs=[chatbot, msg_input])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #demo.launch(server_name=\"0.0.0.0\", server_port=5000, share=False)\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "n7ySt2SxbwpV",
        "outputId": "ed18171e-dab5-46b5-83ff-d5e96f683a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-20-2247015947.py:265: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://745346a372a6851983.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://745346a372a6851983.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fffnsq71pbaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a843dbd",
        "outputId": "4bdfb2a3-0cb7-426d-8c5e-135e3b087696"
      },
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJMyzu69pkZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a812a199",
        "outputId": "d1e4d483-d389-4cd0-9eb6-4d037ee51a7a"
      },
      "source": [
        "!pip install python-docx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m163.8/253.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment - Create an RAG app for multiple files**"
      ],
      "metadata": {
        "id": "DvGPgX99tJ8U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zAl_TVutpwX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}