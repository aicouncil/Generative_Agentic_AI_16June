# -*- coding: utf-8 -*-
"""gen_ai_rag_basics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q5ZGldFFJZaAiFFermaQ7dM9E9B-cYYq
"""

!pip install sentence-transformers

!pip install PyMuPDF

import fitz     #PyMuPDF
from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
#sentence embedding model, good at checking similarity between short text (query vs paragraph)

#function to extract text from uploaded PDF
def extract_data_from_pdf(pdf_path):
  doc = fitz.open(pdf_path)
  full_text = ""
  for page in doc:
    text = page.get_text()
    full_text += text
  return full_text

text = """
Company Manual - TechNova Solutions Pvt. Ltd. About the Company TechNova Solutions Pvt. Ltd. is a global leader in consumer electronics,
committed to delivering innovative technology and excellent customer service. Founded in 2005, TechNova has grown to serve over 20 million
customers across 35 countries. We specialize in smart devices, home automation, and sustainable technology solutions.
Our mission is to make everyday life smarter, simpler, and more connected. We believe in constant innovation, environmental responsibility,
and customer satisfaction above all else. Our headquarters are located in Bengaluru, India, with major regional offices in Singapore, Germany,
and the United States. Return Policy We offer a 30-day return policy on all our products purchased directly through our website or authorized outlets.
To be eligible for a return, the product must be unused, in its original packaging, and accompanied by the original invoice.
Refunds are initiated after we receive and inspect the returned item.
"""

len(text.split())

text_splited = text.split()
' '.join(text_splited[0:0+20])

for i in range(0,10,3):
  print(i)

#function to split text text into sentences/chunks

def split_into_sentences(text, chunk_size = 500):
  sentences = text.split()
  sections = []
  for i in range(0 , len(sentences) , chunk_size):
    section = ' '.join(sentences[i : i+chunk_size])
    sections.append(section)
  return sections

split_into_sentences(extract_data_from_pdf('/content/company_manual.pdf') , chunk_size = 100)

#Cosine similarity
def cosine_similarity(a,b):
  return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))

#Function to search relevant section for a question
def sematic_search(query,sections):
  query_embedding = model.encode([query])[0]
  max_similarity = 0
  relevant_section = None

  for section in sections:
    section_embedding = model.encode([section])[0]
    similarity = cosine_similarity(query_embedding,section_embedding)
    if similarity > max_similarity:
      max_similarity = similarity
      relevant_section = section

    return relevant_section

#Upload the document
pdf_path = '/content/company_manual.pdf'
document_text = extract_data_from_pdf(pdf_path)
sections = split_into_sentences(document_text , chunk_size = 100)

#Ask questions interactively
print("Document loaded, You can as k the question about the document uploaded")
print("Type 'exit' to quit")

while True:
  user_input = input("Ask a question")
  if user_input.lower() in ['exit' , 'quit']:
    break
  relevant_sections = sematic_search(user_input,sections)
  print("Bot:" , relevant_sections)

"""****
****
* MiniLM - returns the entire sections, not exact answers.
* SQuAD2(Question/Answering model) - fune tuned to return specific answer
****
****
"""

from transformers import pipeline

#load extractive question answering model
qa_pipeline = pipeline("question-answering" , model = "deepset/roberta-base-squad2")

context = extract_data_from_pdf('/content/company_manual.pdf')

#Ask questions
while True:
  user_input = input("Ask a question")
  if user_input.lower() in ['exit' , 'quit']:
    break

  result = qa_pipeline({
      'context' : context,
      'question' : user_input
  })

  #print(result['score'])

  if result['score'] >= 0.001:
    print(f"Bot:  {result['answer']}\n")
  else:
    print("Bot: Sorry, I couldn't find a good answer.\n")

"""**Web interface using gradio**
  * Open source Python libraray to create interactive user interface
  * Turn your model inro web-based-app
  * Test or demo your model
  * share your models with non technical users
  * Host or deploy your ML tools online temporarily or permanently
"""

!pip install gradio

import gradio as gr

def add_numbers(num1, num2):
  return num1 + num2

#gradio interface
demo = gr.Interface(
    fn = add_numbers,
    inputs = [gr.Number() , gr.Number()],
    outputs = gr.Number(),
    title = "Simple adder",
    description = "Enter two numbers to add"
)

demo.launch()

"""**User interface with cloud deployment**"""

import gradio as gr
import fitz     #PyMuPDF
from transformers import Pipeline

#load extractive question answering model
qa_pipeline = pipeline("question-answering" , model = "deepset/roberta-base-squad2")

context = ""

#Function to extract text from uploaded pdf
def extract_data_from_pdf(pdf_path):
  global context
  try:
    doc = fitz.open(pdf_path)
    full_text = ""
    for page in doc:
      text = page.get_text()
      full_text += text
    context = full_text
    return "Document uploaded and processed successfully"
  except Exception as err:
    return f"Error Processing the document : {str(err)}"


#Function to answer user's questions based on document
def answer_question(question):
  if not context:
    return "Please upload a document first"
  result = qa_pipeline({
      'context' : context,
      'question' : question
  })

  #print(result['score'])

  if result['score'] >= 0.001:
    print(f"Bot:  {result['answer']}\n")
  else:
    print("Bot: Sorry, I couldn't find a good answer.\n")

import gradio as gr

#gradio interface

   
