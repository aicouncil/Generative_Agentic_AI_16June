{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PPUStZmOGO_",
        "outputId": "e37dfbce-3544-4f34-dcc2-2c8e13e724ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "jiAhU529P_Kw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the text generation pipeline\n",
        "generator = pipeline(\"text-generation\" , model=\"gpt2\")\n",
        "\n",
        "#Generate text\n",
        "prompt = \"In the future, artificial intelligence will\"\n",
        "output = generator(prompt ,\n",
        "                   max_length=50,\n",
        "                   num_return_sequences = 2,\n",
        "                   do_sample = True,\n",
        "                   top_k = 50,\n",
        "                   top_p = 0.95,\n",
        "                   temperature = 0.9,\n",
        "                   eos_token_id = 50256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3iyRl5WQJPE",
        "outputId": "4002e99c-3629-46af-f3e1-aa50c16b0e32"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBu7Kz0jSEdL",
        "outputId": "0b292e1d-c4bd-4d2d-d499-0f3961aa30aa"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be one of those technology that will allow us to use the intelligence of human beings to solve problems that people have never thought of before. It would be extremely important to enable them to solve their problems in our own real world.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[1]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYNr3mhSeDVn",
        "outputId": "8f5051a3-d662-4d42-9e10-d57e03244d3e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to communicate with us to better understand what we need and how we use it. This will enable us to better use human intelligence to help us improve our health and to protect ourselves from disease, injury and illness.\n",
            "\n",
            "The new technology is called the 'Binary Machine'.\n",
            "\n",
            "It is designed to allow humans to communicate using just one brain and a simple form of the language called 'English'. The machine is designed to have a number of different languages, which could be different from humans.\n",
            "\n",
            "Binary machines can be used for all sorts of work: medical work, research and education, engineering work, manufacturing and scientific research.\n",
            "\n",
            "How it works The artificial intelligence is a machine that uses the same types of data as humans but uses a single language. There is no need for the human voice or eyes, and the artificial intelligence takes care of your data needs.\n",
            "\n",
            "The machine has no human voices or eyes and only uses the human language.\n",
            "\n",
            "The machine uses this data to make decisions, and has no need for human input or any other input.\n",
            "\n",
            "Why the Binary Machine is so important A big deal is that it allows human beings to use all kinds of machines. The way we do this is by using complex cognitive processes,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**eos_token_id = 50256**\n",
        "\n",
        "  * End of sequence (stop token) tells the model where to stop generating.\n",
        "  * GPT-2 uses the token-ID 50256 to represent the end of text, if we don't use it it will keep generating the text until the max_length is hit.\n",
        "\n",
        "**do_sample = True**\n",
        "\n",
        "  * It enables the randonmness\n",
        "  * If False => the model always picks the word with the highest probability => very repetitive and boring\n",
        "  * If True => It can explore other likely words.\n",
        "\n",
        "**top_k = 50**\n",
        "\n",
        "    * Controls how many top options to consider when picking the next word\n",
        "    * Pick the next word from top 50 most likely works\n",
        "    * If 1k words are possible next, we narrow it to the top 50 most probable ones, then randomly pick one.\n",
        "\n",
        "**top_p = 0.95**\n",
        "\n",
        "    * Pick from smallest possible set of words whose total probability adds up to 95%\n",
        "\n",
        "**top k is fixed but top_p is flexible based on the context and confidence.**\n",
        "\n",
        "**temperature = 0.7**\n",
        "\n",
        "  * temperature = 1.0 : Normal randomness\n",
        "  * temperature < 1, Less random , more focused\n",
        "  * temperature > 1, More random , more surprising (sometimes may be nonsense)\n",
        "\n",
        "**num_return_sequences = 1**\n",
        "   \n",
        "   * controls number of variations the model should generate for same prompt\n",
        "   * num_return_sequences = 1 -> return only 1 variation.\n",
        "   * num_return_sequences = 3 -> return only 3 different variations of the same prompt.\n",
        "\n"
      ],
      "metadata": {
        "id": "C2QGSN5MUGWl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BK5wPxBZS47z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}